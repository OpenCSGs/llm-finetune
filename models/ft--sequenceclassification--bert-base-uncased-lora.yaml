model_config:
  warmup: True
  model_task: fill-mask
  model_id: bert-base-uncased
  initialization:
    initializer:
      type: Finetune
      dtype: float32
      from_pretrained_kwargs:
        trust_remote_code: true
ft_config:
  # ft_stage: "sft"
  ft_method: "lora"
  ft_task: "sequenceclassification"
  data_config:
    data_path: glue
    subset: mrpc
    local_path: dataset/glue/mrpc/1.0.0
    num_row: 30     # 0: Train with all data.  >0: Test with $num_row data
    input_columns: 
      - "sentence"
    validation_column: validation
  train_config:
    lora_config: 
      r: 1  # Lora attention dimension
      task_type: SEQ_CLS   #SEQ_CLS, SEQ_2_SEQ_LM, CAUSAL_LM, TOKEN_CLS, QUESTION_ANS, FEATURE_EXTRACTION
      lora_alpha: 1  # The alpha parameter for Lora scaling
      lora_dropout: 0.1   # The dropout probability for Lora layers
    base_config:
      checkpoints_output_dir: finetune_models/
      per_device_train_batch_size: 8
      learning_rate: 2e-5
      num_train_epochs: 2
      weight_decay: 0.01
      logging_strategy: steps
      evaluation_strategy: steps
      save_strategy: steps
      save_steps: 100
