model_config:
  warmup: True
  model_task: text-generation
  model_id: THUDM/chatglm3-6b
  max_input_words: 800
  quantization_bit: 4
  initialization:
    # s3_mirror_config:
      # endpoint_url: http://39.107.108.170:9000 # Optinal for custom S3 storage endpoint url 
      # bucket_uri: s3://opt-125m/facemodel/  # Must include hash file with commit id in repo
      # bucket_uri: /root/.cache/huggingface/hub/ZhipuAI/chatglm3-6b/ # Local path of model with hash file
    initializer:
      type: Finetune
      dtype: float32
      from_pretrained_kwargs:
        trust_remote_code: true
ft_config:
  ft_task: "text-generation"
  ft_method: "lora"
  data_config:
    data_path: AdvertiseGen
    local_path: dataset/AdvertiseGen
    num_row: 30     # 0: Train with all data.  >0: Test with $num_row data
    input_columns: 
      - "content"
    validation_column: summary
  train_config:
    lora_config: 
      r: 1  # Lora attention dimension
      task_type: CAUSAL_LM   #SEQ_CLS, SEQ_2_SEQ_LM, CAUSAL_LM, TOKEN_CLS, QUESTION_ANS, FEATURE_EXTRACTION
      lora_alpha: 1  # The alpha parameter for Lora scaling
      lora_dropout: 0.1   # The dropout probability for Lora layers
    base_config:
      max_length: 500
      checkpoints_output_dir: /tmp/finetune
      per_device_train_batch_size: 1
      per_device_eval_batch_size: 1
      learning_rate: 2e-5
      num_train_epochs: 2
      weight_decay: 0.01
      remove_unused_columns: true
      logging_strategy: steps
      evaluation_strategy: steps
      save_strategy: steps
      save_steps: 25
      max_steps: 50
